{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7M1ozNoKT/6LY7B3ymomg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wrhuang/colab/blob/main/SSL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYJ02JDnQiJg"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# dimension\n",
        "dx = 1000\n",
        "dz = 100\n",
        "dy = 10\n",
        "\n",
        "# sample number\n",
        "n1 = 10000\n",
        "n2 = 10000\n",
        "\n",
        "# pretext data generator\n",
        "x1 = Variable(torch.randn(dx, n1))\n",
        "z = x1[:dz, :] + 0.01*torch.normal(torch.zeros(dz, n1),1)\n",
        "\n",
        "# downstream data generator\n",
        "x2 = Variable(torch.randn(dx, n2))\n",
        "y = x2[:dy, :] + 0.01*torch.normal(torch.zeros(dy, n2),1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfftHn39Q8Ey",
        "outputId": "4cfc4fcf-8159-423f-c9f8-b06c5f8d5d4b"
      },
      "source": [
        "# model parameters\n",
        "wc = Variable(torch.FloatTensor(dz, dx), requires_grad = True)\n",
        "lamb = 0.1\n",
        "torch.nn.init.normal_(wc, mean=0, std=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.6097e-02, -6.5981e-01, -9.8760e-01,  ..., -5.9110e-02,\n",
              "          1.5216e-01,  1.2504e+00],\n",
              "        [ 1.1570e+00,  7.3599e-01, -8.4568e-01,  ..., -3.9360e-01,\n",
              "          8.9573e-01,  9.2777e-01],\n",
              "        [-5.9397e-01, -5.7418e-01,  1.6253e+00,  ..., -5.9980e-01,\n",
              "         -1.7669e-02,  3.3198e-01],\n",
              "        ...,\n",
              "        [-1.4049e+00,  8.6978e-01,  8.9504e-04,  ...,  9.1918e-02,\n",
              "          3.0082e-02, -1.0484e-01],\n",
              "        [-1.1414e+00,  4.2471e-02, -5.3894e-01,  ..., -2.4997e-01,\n",
              "         -1.6463e+00, -1.7725e+00],\n",
              "        [-2.3001e-01, -8.7770e-01, -5.6376e-01,  ..., -7.8183e-01,\n",
              "          7.9416e-01,  1.0006e+00]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZnkBdZPRZxO",
        "outputId": "3ba33f6e-7c5e-4ad2-8cc4-b89ed8e0df59"
      },
      "source": [
        "epochs = 20000\n",
        "lr = 10\n",
        "last_loss = 999\n",
        "\n",
        "for i in range(epochs):\n",
        "  i = i+1\n",
        "  c1 = torch.matmul(wc, x1)\n",
        "  c1_T = torch.transpose(c1, 0, 1)\n",
        "  w1 = torch.chain_matmul(torch.pinverse(torch.matmul(c1, c1_T)), c1, torch.transpose(z,0,1))\n",
        "\n",
        "  c2 = torch.matmul(wc, x2)\n",
        "  c2_T = torch.transpose(c2, 0, 1)\n",
        "  w2 = torch.chain_matmul(torch.pinverse(torch.matmul(c2, c2_T)), c2, torch.transpose(y,0,1))\n",
        "\n",
        "  loss1 = torch.norm(torch.matmul(torch.transpose(c1,0,1),w1)-torch.transpose(z,0,1), dim=1)\n",
        "  loss2 = torch.norm(torch.matmul(torch.transpose(c2,0,1),w2)-torch.transpose(y,0,1), dim=1)\n",
        "  loss = torch.mean(loss2) - torch.mean(loss1)*lamb\n",
        "  \n",
        "  if i%100==0:\n",
        "    print(f'epoch={i}, loss={loss}')\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    wc -= lr * wc.grad\n",
        "    wc.grad.zero_()\n",
        "  \n",
        "  if last_loss < loss:\n",
        "    break\n",
        "  last_loss = loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch=100, loss=1.8013184070587158\n",
            "epoch=200, loss=1.4778900146484375\n",
            "epoch=300, loss=0.9365529417991638\n",
            "epoch=400, loss=0.19214481115341187\n",
            "epoch=500, loss=-0.6354119777679443\n",
            "epoch=600, loss=-0.8744691014289856\n",
            "epoch=700, loss=-0.8753929138183594\n",
            "epoch=800, loss=-0.8762966990470886\n",
            "epoch=900, loss=-0.8771812319755554\n",
            "epoch=1000, loss=-0.8780466318130493\n",
            "epoch=1100, loss=-0.8788933753967285\n",
            "epoch=1200, loss=-0.8797216415405273\n",
            "epoch=1300, loss=-0.880531907081604\n",
            "epoch=1400, loss=-0.8813244700431824\n",
            "epoch=1500, loss=-0.8820995688438416\n",
            "epoch=1600, loss=-0.8828575611114502\n",
            "epoch=1700, loss=-0.883599042892456\n",
            "epoch=1800, loss=-0.8843239545822144\n",
            "epoch=1900, loss=-0.8850328326225281\n",
            "epoch=2000, loss=-0.8857259154319763\n",
            "epoch=2100, loss=-0.8864034414291382\n",
            "epoch=2200, loss=-0.887066125869751\n",
            "epoch=2300, loss=-0.8877137899398804\n",
            "epoch=2400, loss=-0.8883469104766846\n",
            "epoch=2500, loss=-0.8889659643173218\n",
            "epoch=2600, loss=-0.8895710706710815\n",
            "epoch=2700, loss=-0.8901625275611877\n",
            "epoch=2800, loss=-0.8907408118247986\n",
            "epoch=2900, loss=-0.8913058638572693\n",
            "epoch=3000, loss=-0.8918583393096924\n",
            "epoch=3100, loss=-0.8923981189727783\n",
            "epoch=3200, loss=-0.8929259777069092\n",
            "epoch=3300, loss=-0.8934416174888611\n",
            "epoch=3400, loss=-0.8939458727836609\n",
            "epoch=3500, loss=-0.8944386839866638\n",
            "epoch=3600, loss=-0.8949202299118042\n",
            "epoch=3700, loss=-0.8953909277915955\n",
            "epoch=3800, loss=-0.8958509564399719\n",
            "epoch=3900, loss=-0.8963005542755127\n",
            "epoch=4000, loss=-0.8967399001121521\n",
            "epoch=4100, loss=-0.8971694111824036\n",
            "epoch=4200, loss=-0.8975892066955566\n",
            "epoch=4300, loss=-0.8979995250701904\n",
            "epoch=4400, loss=-0.8984005451202393\n",
            "epoch=4500, loss=-0.8987925052642822\n",
            "epoch=4600, loss=-0.8991754651069641\n",
            "epoch=4700, loss=-0.8995499610900879\n",
            "epoch=4800, loss=-0.8999159932136536\n",
            "epoch=4900, loss=-0.9002736210823059\n",
            "epoch=5000, loss=-0.9006233215332031\n",
            "epoch=5100, loss=-0.9009650349617004\n",
            "epoch=5200, loss=-0.901299238204956\n",
            "epoch=5300, loss=-0.9016258716583252\n",
            "epoch=5400, loss=-0.9019451141357422\n",
            "epoch=5500, loss=-0.9022570848464966\n",
            "epoch=5600, loss=-0.9025622606277466\n",
            "epoch=5700, loss=-0.9028606414794922\n",
            "epoch=5800, loss=-0.9031522274017334\n",
            "epoch=5900, loss=-0.9034373760223389\n",
            "epoch=6000, loss=-0.9037159085273743\n",
            "epoch=6100, loss=-0.9039885997772217\n",
            "epoch=6200, loss=-0.9042550325393677\n",
            "epoch=6300, loss=-0.9045155644416809\n",
            "epoch=6400, loss=-0.9047703146934509\n",
            "epoch=6500, loss=-0.9050194025039673\n",
            "epoch=6600, loss=-0.9052630662918091\n",
            "epoch=6700, loss=-0.905501127243042\n",
            "epoch=6800, loss=-0.9057341814041138\n",
            "epoch=6900, loss=-0.9059619903564453\n",
            "epoch=7000, loss=-0.9061846137046814\n",
            "epoch=7100, loss=-0.906402587890625\n",
            "epoch=7200, loss=-0.9066157341003418\n",
            "epoch=7300, loss=-0.9068242311477661\n",
            "epoch=7400, loss=-0.9070280194282532\n",
            "epoch=7500, loss=-0.9072275161743164\n",
            "epoch=7600, loss=-0.9074224829673767\n",
            "epoch=7700, loss=-0.9076133966445923\n",
            "epoch=7800, loss=-0.907800018787384\n",
            "epoch=7900, loss=-0.9079827070236206\n",
            "epoch=8000, loss=-0.9081614017486572\n",
            "epoch=8100, loss=-0.9083361029624939\n",
            "epoch=8200, loss=-0.9085071086883545\n",
            "epoch=8300, loss=-0.9086744785308838\n",
            "epoch=8400, loss=-0.9088382124900818\n",
            "epoch=8500, loss=-0.908998429775238\n",
            "epoch=8600, loss=-0.9091551899909973\n",
            "epoch=8700, loss=-0.909308671951294\n",
            "epoch=8800, loss=-0.9094588160514832\n",
            "epoch=8900, loss=-0.9096057415008545\n",
            "epoch=9000, loss=-0.909749448299408\n",
            "epoch=9100, loss=-0.9098902940750122\n",
            "epoch=9200, loss=-0.9100281596183777\n",
            "epoch=9300, loss=-0.9101629257202148\n",
            "epoch=9400, loss=-0.9102948307991028\n",
            "epoch=9500, loss=-0.9104240536689758\n",
            "epoch=9600, loss=-0.910550594329834\n",
            "epoch=9700, loss=-0.9106743335723877\n",
            "epoch=9800, loss=-0.9107955694198608\n",
            "epoch=9900, loss=-0.9109142422676086\n",
            "epoch=10000, loss=-0.9110304713249207\n",
            "epoch=10100, loss=-0.9111440777778625\n",
            "epoch=10200, loss=-0.911255419254303\n",
            "epoch=10300, loss=-0.9113646149635315\n",
            "epoch=10400, loss=-0.911471426486969\n",
            "epoch=10500, loss=-0.911575973033905\n",
            "epoch=10600, loss=-0.9116783738136292\n",
            "epoch=10700, loss=-0.9117786288261414\n",
            "epoch=10800, loss=-0.9118767380714417\n",
            "epoch=10900, loss=-0.9119730591773987\n",
            "epoch=11000, loss=-0.9120672345161438\n",
            "epoch=11100, loss=-0.9121596217155457\n",
            "epoch=11200, loss=-0.9122498631477356\n",
            "epoch=11300, loss=-0.9123383164405823\n",
            "epoch=11400, loss=-0.9124252200126648\n",
            "epoch=11500, loss=-0.9125102162361145\n",
            "epoch=11600, loss=-0.9125933647155762\n",
            "epoch=11700, loss=-0.9126750826835632\n",
            "epoch=11800, loss=-0.9127548336982727\n",
            "epoch=11900, loss=-0.9128330945968628\n",
            "epoch=12000, loss=-0.9129098653793335\n",
            "epoch=12100, loss=-0.9129849076271057\n",
            "epoch=12200, loss=-0.9130585789680481\n",
            "epoch=12300, loss=-0.9131307601928711\n",
            "epoch=12400, loss=-0.9132015109062195\n",
            "epoch=12500, loss=-0.9132707715034485\n",
            "epoch=12600, loss=-0.9133386611938477\n",
            "epoch=12700, loss=-0.9134052395820618\n",
            "epoch=12800, loss=-0.9134705662727356\n",
            "epoch=12900, loss=-0.9135343432426453\n",
            "epoch=13000, loss=-0.9135971069335938\n",
            "epoch=13100, loss=-0.9136584401130676\n",
            "epoch=13200, loss=-0.9137187004089355\n",
            "epoch=13300, loss=-0.9137776494026184\n",
            "epoch=13400, loss=-0.9138355255126953\n",
            "epoch=13500, loss=-0.9138922691345215\n",
            "epoch=13600, loss=-0.9139478802680969\n",
            "epoch=13700, loss=-0.9140022993087769\n",
            "epoch=13800, loss=-0.9140558242797852\n",
            "epoch=13900, loss=-0.914108157157898\n",
            "epoch=14000, loss=-0.9141595959663391\n",
            "epoch=14100, loss=-0.9142101407051086\n",
            "epoch=14200, loss=-0.9142593145370483\n",
            "epoch=14300, loss=-0.9143077731132507\n",
            "epoch=14400, loss=-0.9143552780151367\n",
            "epoch=14500, loss=-0.9144018888473511\n",
            "epoch=14600, loss=-0.9144474864006042\n",
            "epoch=14700, loss=-0.9144924283027649\n",
            "epoch=14800, loss=-0.9145362973213196\n",
            "epoch=14900, loss=-0.914579451084137\n",
            "epoch=15000, loss=-0.9146216511726379\n",
            "epoch=15100, loss=-0.9146631360054016\n",
            "epoch=15200, loss=-0.9147037267684937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNrSGWhcmeT3",
        "outputId": "fb898b29-f5f5-4ac1-c550-134f8ae8b32d"
      },
      "source": [
        "wc.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 1000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo8B-2w0XUsB",
        "outputId": "9951b2d0-d381-4bb0-ab37-69da4c7267b3"
      },
      "source": [
        "torch.norm(wc[:,0:9],dim=0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([8.5263, 9.0020, 8.7312, 9.0478, 9.2518, 8.9538, 9.6542, 9.3752, 9.6234],\n",
              "       grad_fn=<CopyBackwards>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLnXZeLHXYKn",
        "outputId": "32a94fd5-2087-4125-cce4-6e272702c440"
      },
      "source": [
        "torch.norm(wc[:,10:99],dim=0)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.8541, 3.0973, 2.9945, 2.6902, 2.9489, 2.8095, 3.4668, 3.1792, 2.9857,\n",
              "        2.7453, 2.9109, 2.6606, 3.1700, 2.6737, 2.7873, 3.0646, 2.9446, 3.1335,\n",
              "        2.8279, 2.8779, 3.1074, 3.0287, 3.3657, 2.9438, 3.0469, 2.7410, 3.4830,\n",
              "        3.5034, 3.1896, 2.6452, 2.9683, 2.7309, 3.2930, 3.2287, 3.0535, 2.6696,\n",
              "        3.0652, 2.9075, 3.0472, 3.0646, 2.8729, 3.0404, 3.2504, 3.1973, 3.4336,\n",
              "        3.1482, 2.9991, 3.3161, 2.7615, 3.0308, 2.7638, 3.3246, 3.2844, 3.0035,\n",
              "        2.7270, 3.2018, 3.1896, 3.0827, 2.7565, 3.3848, 2.9844, 3.1788, 2.7677,\n",
              "        3.3893, 3.1123, 3.1816, 2.5607, 2.7820, 2.8995, 2.8415, 2.9883, 2.9450,\n",
              "        2.7592, 2.6565, 3.3650, 2.8204, 2.7743, 3.0673, 2.8414, 3.2894, 3.3344,\n",
              "        2.7996, 3.1112, 2.9558, 3.0691, 2.9263, 3.0787, 2.9028, 3.4751],\n",
              "       grad_fn=<CopyBackwards>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVv1NinAqSWO",
        "outputId": "338ee502-7460-4e9c-8b1a-9b301e3491a7"
      },
      "source": [
        "torch.norm(wc[:,100:999],dim=0)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10.6303, 10.5668, 10.6948, 10.9328, 10.3416, 10.7403, 10.5056, 11.4694,\n",
              "        10.4560, 10.9550, 11.5283,  9.6686, 11.0914, 10.3341, 11.3453, 10.8139,\n",
              "        10.3907, 10.5352, 11.4074,  9.1758, 11.6134, 10.7069, 10.3322,  8.9797,\n",
              "         9.6399, 10.4763, 10.4705, 10.2820,  9.5484,  9.8416,  9.6571, 11.0201,\n",
              "        10.6916, 10.3296, 10.9799, 10.0771, 10.9470, 10.0145, 10.5150, 10.5857,\n",
              "        10.4242, 10.1496, 10.0800,  9.2742,  9.9713, 11.0098, 10.7988, 10.6680,\n",
              "        10.1006, 10.2039,  9.5419, 10.0611,  9.8971,  8.7866, 10.2681, 10.7606,\n",
              "        11.6471,  9.6636, 11.0798, 10.4409,  9.7128, 10.1463, 11.1101,  9.3904,\n",
              "        10.1379, 10.8966, 11.4554, 10.2444, 10.4791, 10.7828, 11.0819,  9.8814,\n",
              "         9.8619, 10.7827, 10.1247,  9.9842, 10.5417, 11.1089, 10.2675, 10.2127,\n",
              "         9.9870,  9.1812, 10.3596, 10.6315, 10.0574, 10.4935, 11.8075, 10.5540,\n",
              "         9.7678, 10.3813,  9.9816, 10.4555, 10.4669, 10.1164, 10.2813,  9.6987,\n",
              "        10.4999, 10.9769, 11.0314,  9.6162, 10.9668, 11.0153, 10.4160, 11.0191,\n",
              "        10.3398, 10.1456, 10.9172, 10.9550, 10.1310,  9.7789, 10.1247,  9.7476,\n",
              "        10.9065, 10.2010, 10.5121, 11.6426, 12.1385,  9.4524,  9.4857, 10.0072,\n",
              "        10.2452, 11.9330, 10.1809,  9.9217, 10.0483, 10.8700, 10.1055,  9.6794,\n",
              "        10.5745, 10.1902,  9.0205, 10.4774, 10.6849,  9.4106, 11.1876,  9.3786,\n",
              "         9.5064, 11.5360, 10.9679, 10.4381, 10.1810, 10.6392, 10.3967, 11.4229,\n",
              "        11.3778, 11.0234, 11.0153, 10.9862, 10.0453, 10.8058, 10.7645, 11.5943,\n",
              "        10.9170, 10.7303, 10.9616, 10.3945, 10.0873, 10.1605, 10.8548, 11.3064,\n",
              "        10.7685, 12.2457, 10.4394, 10.8345, 11.1042,  9.8202, 11.5236, 10.3080,\n",
              "         9.4279,  9.3937,  9.1417, 10.2065,  9.1237, 10.2283, 10.5085, 10.6451,\n",
              "         9.7794,  9.8593, 11.3315, 11.0825, 10.3533, 11.1625,  9.6549, 10.5488,\n",
              "        11.2134, 10.5073, 11.0614, 10.5163, 10.8925,  9.7831, 10.6398, 10.6264,\n",
              "        10.1912, 11.4712,  9.9699, 10.6186, 10.9742, 10.4160, 10.0360, 10.4788,\n",
              "        10.5085, 10.4220, 11.0470, 10.3885, 10.2336, 10.5948,  9.7767,  9.3748,\n",
              "        11.2532, 10.3770, 10.9868, 10.0043,  9.7895, 10.4841,  9.9893, 10.4074,\n",
              "        10.6101, 10.1560, 11.3270, 11.3102,  9.6451, 10.3096,  9.7012,  9.3598,\n",
              "        11.2811, 11.1043, 10.2811, 10.6937, 10.2018, 10.4774, 10.6177, 10.7269,\n",
              "        10.8835, 10.4446, 11.5578, 11.4750, 10.5607, 10.8671, 10.5769,  9.2675,\n",
              "         9.9125,  8.9696,  9.9353, 11.8156, 11.0182, 10.8639, 11.0021, 10.2571,\n",
              "        10.5694, 10.4195,  9.9922, 11.0148,  9.4300, 10.7667, 10.6187,  9.1333,\n",
              "        10.5494,  9.7735, 10.3235,  9.9531, 10.0511, 11.8998, 10.7807,  9.7901,\n",
              "        11.0009, 11.1075, 12.0640, 10.0927,  9.9713, 12.0970, 10.4721, 10.2941,\n",
              "        10.0591, 10.6584, 12.2958, 10.5269, 12.3266, 11.3827, 10.9871, 10.7692,\n",
              "        10.6011, 10.0170, 10.4478, 10.5376, 10.0528, 10.9762, 11.1134, 11.8837,\n",
              "        10.3273, 10.5284, 10.3107, 10.5933, 10.9710, 10.6802, 10.1745,  9.7060,\n",
              "        10.0810, 10.5281, 10.7830, 10.2705, 10.4178, 11.6381, 11.7804, 10.6038,\n",
              "        10.1724, 10.1248, 10.2671, 11.6491, 10.3589, 10.3978, 10.4417, 11.8150,\n",
              "        10.9104, 10.7483, 10.7173, 11.7639, 11.3110, 10.3975,  9.6694, 11.0800,\n",
              "        11.7275, 10.3917, 10.5371, 10.6129, 10.3767, 11.0185, 10.8404, 10.7311,\n",
              "        11.6339,  8.8275,  8.7883,  9.6597, 10.4822, 10.4709, 11.2144,  9.7200,\n",
              "        10.2990, 10.1953, 11.2343, 10.3799, 10.1206,  9.6690, 11.0879,  9.5782,\n",
              "        10.3035, 10.1955,  9.8660, 10.7891,  9.4720, 11.5386, 10.1819, 10.4142,\n",
              "         9.5059,  9.8160, 10.1239, 10.3285, 10.1492, 10.3919, 11.1318, 10.4593,\n",
              "        10.1191, 10.1013, 11.0420, 10.3448,  9.9200, 12.9104, 11.4343, 10.3513,\n",
              "        10.3253, 11.1853, 10.6350, 10.2361,  9.5930, 11.3584, 10.0896, 11.5354,\n",
              "        10.6677, 11.1364, 11.8315, 11.2387, 10.4785, 11.7984, 10.5804, 10.3899,\n",
              "         9.8541,  9.6082, 10.3314, 10.5362,  9.9908, 11.5156, 12.4439, 12.2627,\n",
              "         9.8109, 10.5378, 11.7697,  9.8806,  9.5240, 10.7621, 10.4284, 10.2997,\n",
              "        10.0062, 10.4529, 11.6314,  9.8170, 10.9533,  9.6059, 10.6594, 11.1326,\n",
              "        10.7333,  9.8335, 10.4258, 10.6631,  9.7105,  9.9649, 11.2119, 10.9456,\n",
              "        11.1082, 11.0614, 11.3634, 11.8019, 10.4362,  9.9332, 10.8463, 10.0133,\n",
              "         9.8730, 11.9607,  9.9168, 10.5198, 11.1273, 10.5616, 10.7594, 12.3578,\n",
              "         9.5312,  9.6807, 10.1671,  9.5055, 11.4920, 10.8142, 10.4887,  9.8880,\n",
              "         9.8815, 10.3795,  9.4741, 10.8871, 10.3031,  9.9683, 10.0726, 10.5029,\n",
              "         9.5385,  9.7004, 10.8966, 10.5652,  9.8768, 10.7037,  9.3847,  9.7962,\n",
              "         9.6197,  9.9160, 10.7700, 10.7456, 10.6410, 10.3502, 10.2477, 11.1020,\n",
              "        11.3974,  9.2891,  9.3900, 10.5828,  9.3706,  9.6685, 10.5703, 10.5577,\n",
              "        11.1359, 11.0278, 10.4961,  9.8962, 10.0552, 10.9243, 11.4039, 11.1802,\n",
              "         9.9630, 10.7250, 10.8723, 10.9539, 10.9370, 11.2286,  9.8353, 11.0154,\n",
              "        10.7449,  9.8813, 10.6308,  9.6103, 10.8869,  9.8250, 10.9382, 10.9186,\n",
              "        10.6817, 11.9557, 10.4300, 10.2715, 10.7595, 10.4389, 10.5335, 11.1613,\n",
              "        10.2704,  9.7958, 11.1884,  9.8625, 10.0884, 10.1409,  9.6504,  9.5896,\n",
              "        10.5190, 10.8328, 11.1391, 10.6630,  9.6890, 10.1818, 10.1340, 10.5600,\n",
              "        10.5162,  9.4840, 10.1760,  9.9409, 10.1919, 10.9549, 10.0087,  9.9595,\n",
              "        11.2490,  9.6358, 10.0984, 10.3202, 10.1745,  9.9857,  8.7372,  9.0580,\n",
              "        10.3641, 10.0381,  9.3415, 11.6978,  9.9723,  9.9921, 10.3187, 10.6288,\n",
              "         9.6360,  8.9888, 10.5907, 10.0221, 11.3442, 11.0000, 11.6547, 11.9702,\n",
              "         9.7342, 11.0492, 10.4053, 10.0864, 11.5347, 10.5438, 10.6974, 11.0982,\n",
              "         9.8968,  9.8457, 10.7540, 10.8175, 10.0497, 10.7013, 10.2290, 11.1295,\n",
              "        10.7630,  9.9811, 10.4495,  9.5486, 10.2853, 11.9950, 10.8175, 10.3298,\n",
              "         9.9695, 10.0932, 11.1583, 10.4368,  9.9401, 10.0685,  9.7137,  9.2627,\n",
              "        10.5097, 10.4226, 10.4288, 11.3562, 10.0647, 10.1516,  9.8194, 10.9091,\n",
              "        10.4555,  9.4216, 10.3477,  9.5703, 10.9840,  8.9480,  9.7524, 10.7477,\n",
              "        10.1293, 10.0539,  9.1636, 10.1333, 10.7704, 11.0422,  9.5227, 10.8670,\n",
              "        10.4444, 11.0574,  9.9114, 10.7563, 10.7366, 10.5760, 10.7651, 10.8862,\n",
              "         9.4496,  9.6174, 10.8648, 10.5007,  9.7604,  9.8214, 10.7262,  9.2611,\n",
              "         9.3814,  9.6070, 10.2100, 11.3732, 11.5939, 10.7812,  9.0730, 10.4419,\n",
              "        11.2526, 10.4371, 11.0774,  9.6011, 10.1230, 10.4818, 10.6376,  9.3257,\n",
              "        10.9197,  9.0349, 10.4006, 10.7248, 11.0491, 10.1341, 12.0001, 10.3782,\n",
              "        10.3091, 10.6691, 10.0611,  9.7711, 10.5095,  9.8137, 10.2952, 11.1956,\n",
              "        10.8376, 10.7388, 10.3088, 11.0027, 10.1066, 11.0614, 10.6471,  9.7742,\n",
              "        10.7116,  9.2096,  9.8087, 11.0873,  9.6677, 11.2180, 10.5459, 10.8440,\n",
              "        10.5478,  8.8657, 10.2120,  9.8489, 11.3454, 10.1729, 10.6318,  9.6249,\n",
              "        10.8647, 10.6824,  9.5819, 10.7076, 10.9351, 10.3454,  9.8795, 10.7051,\n",
              "        10.4188, 10.3713, 10.3740, 11.0255, 10.2715,  9.5345,  9.3776, 11.2254,\n",
              "        10.8622,  9.4797, 12.2072, 11.5669, 11.3381, 10.3607, 11.2444, 11.0519,\n",
              "        11.2052,  9.5093, 10.1924, 10.6151, 10.6670, 11.5678, 10.2743,  9.7425,\n",
              "        10.6411,  9.8256, 10.5212,  9.1886, 11.3395, 10.5216, 11.3263, 10.1011,\n",
              "        10.2146, 11.4201, 11.2795,  9.0112, 10.9223, 10.7317, 10.3466, 11.9870,\n",
              "        10.2782, 10.1045, 10.7184, 10.7648, 10.4988, 10.2514,  9.8664, 11.6727,\n",
              "        10.8465, 10.5639, 10.2201, 11.3554, 10.2827, 10.8442, 10.9606, 11.0454,\n",
              "        10.7706, 10.3875, 10.5004, 10.6226,  9.8454,  8.9332,  9.8475, 10.0776,\n",
              "        10.6598, 10.7291, 10.4492, 10.3628, 10.2404, 10.0332,  9.9406, 10.9186,\n",
              "         9.3154,  9.3345, 10.7808, 11.2306, 11.0594, 10.6907, 10.1523,  9.7872,\n",
              "        10.0388, 10.3682, 10.7027, 10.7756,  9.2007, 10.6139, 10.1679, 10.6385,\n",
              "         9.7258, 10.3454, 10.2584, 10.2918,  9.9260, 10.1115, 10.5070, 10.8418,\n",
              "         9.4523, 11.3317,  9.2305, 10.3923, 10.0107, 10.4707, 10.6321,  9.2867,\n",
              "        10.8564,  9.7057, 10.5005, 11.8602, 10.5291, 10.0082, 10.2545,  9.1662,\n",
              "        10.3993, 10.4342, 10.5807, 12.3379, 11.0614, 10.9780, 10.3562, 10.6430,\n",
              "        11.2529,  9.9760, 10.5511, 10.6957, 10.3487,  9.9765,  9.5374, 11.1921,\n",
              "        10.5466,  9.4943,  9.4963, 10.7804, 10.0218, 10.8394,  9.9562, 10.6544,\n",
              "         9.9342, 10.3176, 10.9597,  9.3578, 10.4704, 10.9825, 11.4825, 11.5862,\n",
              "         9.4567, 10.8174, 10.7088, 10.9508,  9.2133, 11.3429,  9.8011, 10.1954,\n",
              "        10.6444, 10.2205,  9.5713, 10.1441, 11.4144,  9.6296, 10.4996, 10.9861,\n",
              "        10.3968, 11.2880, 10.8964, 10.3611, 11.0115, 10.9209, 10.3648, 10.6147,\n",
              "        10.9096,  9.9158, 11.4498, 13.2362, 10.2689,  9.8663, 10.3035, 11.4880,\n",
              "        10.8536, 11.3477, 11.1405, 10.4891, 11.3445,  9.3238,  9.7339, 10.6347,\n",
              "        11.1522,  9.7343, 10.1165,  9.0622,  9.1962,  9.5266, 10.5549, 10.4262,\n",
              "        11.0399, 11.2197, 10.4178, 10.0372, 10.9188, 10.6150,  9.9551,  9.5104,\n",
              "        10.8709,  9.0884,  9.7728, 10.2918,  9.8845, 10.2073, 10.8683, 11.3497,\n",
              "        10.1171, 10.6114, 10.6792], grad_fn=<CopyBackwards>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}